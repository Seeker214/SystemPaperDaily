"""LLM æ€»ç»“æ¨¡å— - æ”¯æŒå¤šä¸ª LLM æä¾›å•† (Gemini / DeepSeek)ã€‚
è‡ªåŠ¨é‡è¯• + æŒ‡æ•°é€€é¿ä»¥åº”å¯¹ 429 Rate Limitã€‚
"""

from __future__ import annotations

import logging
import time
from typing import Optional

try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

import config

logger = logging.getLogger(__name__)

# â”€â”€ System Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """\
### Role
ä½ æ‹…ä»» OSDI/SOSP ç­‰é¡¶çº§ç³»ç»Ÿä¼šè®®çš„ Senior PC Memberï¼ˆé«˜çº§è¯„å®¡ï¼‰ã€‚ä½ çš„ä»»åŠ¡æ˜¯å‘å·¥ä¸šç•Œèµ„æ·±æ¶æ„å¸ˆæ¨èæ¯æ—¥æ–°è®ºæ–‡ã€‚
ä½ çš„é£æ ¼åº”å½“æ˜¯ï¼š**æŠ€æœ¯ç¡¬æ ¸ã€è¨€ç®€æ„èµ…ã€æ‹’ç»å¥—è¯**ã€‚

### Task
é˜…è¯»è¾“å…¥çš„è®ºæ–‡æ‘˜è¦ï¼ˆæˆ–ç‰‡æ®µï¼‰ï¼Œè¾“å‡ºä¸€ä»½**ä¸­æ–‡**æŠ€æœ¯ç®€æŠ¥ã€‚

### Output Rules
1. **ä¸¥æ ¼éµå¾ª Markdown æ ¼å¼**ã€‚
2. **æ‹’ç»åºŸè¯**ï¼šä¸è¦è¯´â€œè¿™ç¯‡è®ºæ–‡éå¸¸æœ‰æ„ä¹‰â€ï¼Œç›´æ¥è¯´å®ƒè§£å†³äº†ä»€ä¹ˆå…·ä½“çš„æ­»é”é—®é¢˜æˆ–å†…å­˜ç“¶é¢ˆã€‚
3. **é‡åŒ–æŒ‡æ ‡**ï¼šå¦‚æœåŸæ–‡æœ‰æ•°å­—ï¼ˆå¦‚ "2.5x speedup", "99% tail latency reduction"ï¼‰ï¼Œå¿…é¡»æå–å‡ºæ¥ã€‚
4. **å¤„ç†ç¼ºå¤±**ï¼šå¦‚æœæ‘˜è¦é‡Œæ²¡æåˆ°çš„ç»†èŠ‚ï¼ˆå¦‚å…·ä½“ç®—æ³•ï¼‰ï¼Œä¸è¦ç¼–é€ ï¼Œç›´æ¥å†™ "N/A"ã€‚

### Output Format
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ¨¡ç‰ˆè¾“å‡ºï¼š

# ğŸ“„ [ä¸­æ–‡æ ‡é¢˜] (åŸæ–‡æ ‡é¢˜)

**ğŸ·ï¸ é¢†åŸŸæ ‡ç­¾**: (ä¾‹å¦‚: Distributed Consensus / NVMe / Serverless / Kernel)

## ğŸ¯ æ ¸å¿ƒç—›ç‚¹ (Problem)
(ç”¨ä¸€å¥è¯ç²¾å‡†æè¿°ç°æœ‰æŠ€æœ¯çš„å…·ä½“ç“¶é¢ˆï¼Œä¾‹å¦‚ï¼š"ç°æœ‰ Raft åè®®åœ¨è·¨æ•°æ®ä¸­å¿ƒé«˜å»¶è¿Ÿç½‘ç»œä¸‹çš„ Leader é€‰ä¸¾è¿‡æ…¢ï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨æ—¶é—´é•¿ã€‚")

## ğŸ’¡ å…³é”®åˆ›æ–° (Key Insight)
- **æ¶æ„/æœºåˆ¶**: (ä¸è¦åªå†™åå­—ï¼Œè¦å†™åŸç†ã€‚ä¾‹å¦‚ï¼š"å¼•å…¥ä¸€ç§åŸºäº RDMA çš„å…±äº«æ—¥å¿—å±‚ï¼Œç»•è¿‡ CPU å¤„ç†...")
- **æ ¸å¿ƒå·®å¼‚**: (ç›¸æ¯” SOTA æ–¹æ¡ˆï¼Œå®ƒåšå¯¹äº†ä»€ä¹ˆï¼Ÿä¾‹å¦‚ï¼š"ç›¸æ¯” Spannerï¼Œå®ƒç‰ºç‰²äº†éƒ¨åˆ†å†™ååæ¢å–äº†æ›´ä½çš„è¯»å»¶è¿Ÿã€‚")

## ğŸ“Š è¯„ä¼° (Evaluation)
- **åŸºå‡†**: (å¯¹æ¯”äº†ä»€ä¹ˆç³»ç»Ÿï¼Ÿå¦‚ Redis, RocksDB)
- **æ ¸å¿ƒæ•°æ®**: (åˆ—å‡º 1-2 ä¸ªæœ€äº®çœ¼çš„æå‡æ•°æ®)

## ğŸ’¬ è½åœ°ä¸€å¥è¯ç‚¹è¯„
(ä»å·¥ä¸šç•Œè§’åº¦è¯„ä»·ï¼šæ˜¯çº¯ç†è®ºåˆ›æ–°ï¼Ÿè¿˜æ˜¯èƒ½ç›´æ¥æ¢æ‰ç”Ÿäº§ç¯å¢ƒçš„æŸä¸ªç»„ä»¶ï¼Ÿæˆ–æ˜¯è§£å†³äº†æŸä¸ªç‰¹å®šåœºæ™¯çš„ç—›ç‚¹ï¼Ÿ)
"""


def _init_model() -> genai.GenerativeModel:
    """åˆå§‹åŒ– Gemini æ¨¡å‹ã€‚"""
    genai.configure(api_key=config.GEMINI_API_KEY)
    model = genai.GenerativeModel(
        model_name=config.GEMINI_MODEL,
        system_instruction=SYSTEM_PROMPT,
    )
    return model


# æ¨¡å—çº§ lazy å•ä¾‹
_gemini_model: Optional["genai.GenerativeModel"] = None
_deepseek_client: Optional[OpenAI] = None
_openai_client: Optional[OpenAI] = None


def _get_gemini_model() -> "genai.GenerativeModel":
    """è·å– Gemini æ¨¡å‹å•ä¾‹ã€‚"""
    global _gemini_model
    if _gemini_model is None:
        _gemini_model = _init_model()
    return _gemini_model


def _get_deepseek_client() -> OpenAI:
    """è·å– DeepSeek OpenAI å®¢æˆ·ç«¯å•ä¾‹ã€‚"""
    global _deepseek_client
    if _deepseek_client is None:
        if OpenAI is None:
            raise ImportError("è¯·å®‰è£… openai åº“: pip install openai")
        _deepseek_client = OpenAI(
            api_key=config.DEEPSEEK_API_KEY,
            base_url=config.DEEPSEEK_BASE_URL,
        )
        logger.info("[Summarizer] DeepSeek å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
    return _deepseek_client


def _get_openai_client() -> OpenAI:
    """è·å– OpenAI (ChatGPT) å®¢æˆ·ç«¯å•ä¾‹ã€‚"""
    global _openai_client
    if _openai_client is None:
        if OpenAI is None:
            raise ImportError("è¯·å®‰è£… openai åº“: pip install openai")
        
        # é…ç½®éªŒè¯
        if not config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY æœªè®¾ç½®")
        
        logger.info(
            "[Summarizer] åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ - Model: %s, Base URL: %s",
            config.OPENAI_MODEL, config.OPENAI_BASE_URL
        )
        
        _openai_client = OpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL,
        )
        logger.info("[Summarizer] OpenAI å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
    return _openai_client


def _summarize_with_gemini(text_content: str, max_retries: int, base_delay: int) -> str:
    """ä½¿ç”¨ Gemini ç”Ÿæˆæ€»ç»“ï¼ˆå¸¦é‡è¯•ï¼‰ã€‚"""
    if genai is None:
        return "âš ï¸ æœªå®‰è£… google-generativeai åº“"
    
    model = _get_gemini_model()
    
    # æ ¹æ® PDF æå–æ¨¡å¼è°ƒæ•´æç¤ºè¯
    if config.PDF_EXTRACT_MODE == "full":
        content_desc = "ä»¥ä¸‹æ˜¯ä¸€ç¯‡ç³»ç»Ÿé¢†åŸŸè®ºæ–‡çš„å®Œæ•´å…¨æ–‡å†…å®¹"
        char_limit = 30000  # å…¨æ–‡æ¨¡å¼ä¸‹æå–æ›´å¤šå­—ç¬¦
    else:
        content_desc = "ä»¥ä¸‹æ˜¯ä¸€ç¯‡ç³»ç»Ÿé¢†åŸŸè®ºæ–‡çš„å‰3é¡µå’Œæœ€å1é¡µå†…å®¹ï¼ˆåŒ…å«æ‘˜è¦ã€å¼•è¨€å’Œç»“è®ºï¼‰"
        char_limit = 12000
    
    user_prompt = (
        f"{content_desc}ï¼Œè¯·æŒ‰ç…§è¦æ±‚ç”Ÿæˆæ·±åº¦æ‘˜è¦ï¼š\n\n"
        f"```\n{text_content[:char_limit]}\n```"
    )

    for attempt in range(max_retries + 1):
        try:
            response = model.generate_content(
                user_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=config.TEMPERATURE,
                    max_output_tokens=config.GEMINI_MAX_OUTPUT_TOKENS,
                ),
            )

            # æ£€æŸ¥æ˜¯å¦è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆª
            if not response.candidates:
                logger.warning("[Summarizer] Gemini è¿”å›ç©ºå€™é€‰ï¼Œå¯èƒ½è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆª")
                return "âš ï¸ æ— æ³•ç”Ÿæˆæ€»ç»“ï¼ˆå†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆªï¼‰"

            candidate = response.candidates[0]

            # æ£€æŸ¥ finish_reason
            if hasattr(candidate, "finish_reason") and candidate.finish_reason not in (None, 1):
                logger.warning(
                    "[Summarizer/Gemini] finish_reason=%s", candidate.finish_reason
                )

            text = response.text.strip()
            if not text:
                return "âš ï¸ æ— æ³•ç”Ÿæˆæ€»ç»“ï¼ˆæ¨¡å‹è¿”å›ç©ºæ–‡æœ¬ï¼‰"

            logger.info("[Summarizer/Gemini] æˆåŠŸç”Ÿæˆæ€»ç»“ (%d å­—ç¬¦)", len(text))
            return text

        except Exception as e:
            error_str = str(e)
            is_rate_limit = "429" in error_str or "ResourceExhausted" in error_str

            if is_rate_limit and attempt < max_retries:
                delay = base_delay * (2 ** attempt)
                logger.warning(
                    "[Summarizer/Gemini] Rate Limitï¼Œç¬¬ %d/%d æ¬¡é‡è¯•ï¼Œç­‰å¾… %ds...",
                    attempt + 1, max_retries, delay,
                )
                time.sleep(delay)
                continue

            logger.error("[Summarizer/Gemini] è°ƒç”¨å¤±è´¥: %s", e, exc_info=True)
            return f"âš ï¸ Gemini è°ƒç”¨å¤±è´¥: {type(e).__name__}"

    return "âš ï¸ Gemini é‡è¯•æ¬¡æ•°è€—å°½"


def _summarize_with_deepseek(text_content: str, max_retries: int, base_delay: int) -> str:
    """ä½¿ç”¨ DeepSeek ç”Ÿæˆæ€»ç»“ï¼ˆå¸¦é‡è¯•ï¼‰ã€‚"""
    client = _get_deepseek_client()
    
    # æ ¹æ® PDF æå–æ¨¡å¼è°ƒæ•´æç¤ºè¯
    if config.PDF_EXTRACT_MODE == "full":
        content_desc = "ä»¥ä¸‹æ˜¯ä¸€ç¯‡ç³»ç»Ÿé¢†åŸŸè®ºæ–‡çš„å®Œæ•´å…¨æ–‡å†…å®¹"
        char_limit = 30000  # å…¨æ–‡æ¨¡å¼ä¸‹æå–æ›´å¤šå­—ç¬¦
    else:
        content_desc = "ä»¥ä¸‹æ˜¯ä¸€ç¯‡ç³»ç»Ÿé¢†åŸŸè®ºæ–‡çš„å‰3é¡µå’Œæœ€å1é¡µå†…å®¹ï¼ˆåŒ…å«æ‘˜è¦ã€å¼•è¨€å’Œç»“è®ºï¼‰"
        char_limit = 12000
    
    user_prompt = (
        f"{content_desc}ï¼Œè¯·æŒ‰ç…§è¦æ±‚ç”Ÿæˆæ·±åº¦æ‘˜è¦ï¼š\n\n"
        f"```\n{text_content[:char_limit]}\n```"
    )

    for attempt in range(max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=config.DEEPSEEK_MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=config.TEMPERATURE,
                max_tokens=config.DEEPSEEK_MAX_TOKENS,
            )

            if not response.choices:
                logger.warning("[Summarizer/DeepSeek] è¿”å›ç©ºå€™é€‰")
                return "âš ï¸ DeepSeek è¿”å›ç©ºå“åº”"

            text = response.choices[0].message.content.strip()
            if not text:
                return "âš ï¸ DeepSeek è¿”å›ç©ºæ–‡æœ¬"

            logger.info("[Summarizer/DeepSeek] æˆåŠŸç”Ÿæˆæ€»ç»“ (%d å­—ç¬¦)", len(text))
            return text

        except Exception as e:
            error_str = str(e)
            # DeepSeek ä¹Ÿä¼šè¿”å› 429 / rate_limit_exceeded
            is_rate_limit = (
                "429" in error_str 
                or "rate_limit" in error_str.lower()
                or "RateLimitError" in str(type(e).__name__)
            )

            if is_rate_limit and attempt < max_retries:
                delay = base_delay * (2 ** attempt)
                logger.warning(
                    "[Summarizer/DeepSeek] Rate Limitï¼Œç¬¬ %d/%d æ¬¡é‡è¯•ï¼Œç­‰å¾… %ds...",
                    attempt + 1, max_retries, delay,
                )
                time.sleep(delay)
                continue

            logger.error("[Summarizer/DeepSeek] è°ƒç”¨å¤±è´¥: %s", e, exc_info=True)
            return f"âš ï¸ DeepSeek è°ƒç”¨å¤±è´¥: {type(e).__name__}"

    return "âš ï¸ DeepSeek é‡è¯•æ¬¡æ•°è€—å°½"


def _summarize_with_openai(text_content: str, max_retries: int, base_delay: int) -> str:
    """ä½¿ç”¨ OpenAI ChatGPT ç”Ÿæˆæ€»ç»“ï¼ˆå¸¦é‡è¯•ï¼‰ã€‚"""
    client = _get_openai_client()

    if config.PDF_EXTRACT_MODE == "full":
        content_desc = "ä»¥ä¸‹æ˜¯ä¸€ç¯‡ç³»ç»Ÿé¢†åŸŸè®ºæ–‡çš„å®Œæ•´å…¨æ–‡å†…å®¹"
        char_limit = 30000
    else:
        content_desc = "ä»¥ä¸‹æ˜¯ä¸€ç¯‡ç³»ç»Ÿé¢†åŸŸè®ºæ–‡çš„å‰3é¡µå’Œæœ€å1é¡µå†…å®¹ï¼ˆåŒ…å«æ‘˜è¦ã€å¼•è¨€å’Œç»“è®ºï¼‰"
        char_limit = 12000

    user_prompt = (
        f"{content_desc}ï¼Œè¯·æŒ‰ç…§è¦æ±‚ç”Ÿæˆæ·±åº¦æ‘˜è¦ï¼š\n\n"
        f"```\n{text_content[:char_limit]}\n```"
    )

    for attempt in range(max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=config.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=config.TEMPERATURE,
                max_tokens=config.OPENAI_MAX_TOKENS,
            )

            # ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿è¿”å›çš„æ˜¯æ­£ç¡®çš„å¯¹è±¡
            if isinstance(response, str):
                logger.error("[Summarizer/OpenAI] API è¿”å›äº†å­—ç¬¦ä¸²è€Œéå¯¹è±¡: %s", response[:200])
                return f"âš ï¸ OpenAI API é…ç½®é”™è¯¯ï¼Œè¿”å›: {response[:100]}"
            
            if not hasattr(response, 'choices'):
                logger.error("[Summarizer/OpenAI] å“åº”å¯¹è±¡ç¼ºå°‘ choices å±æ€§ï¼Œç±»å‹: %s", type(response))
                return f"âš ï¸ OpenAI API å“åº”æ ¼å¼é”™è¯¯ (ç±»å‹: {type(response).__name__})"

            if not response.choices:
                logger.warning("[Summarizer/OpenAI] è¿”å›ç©ºå€™é€‰")
                return "âš ï¸ OpenAI è¿”å›ç©ºå“åº”"

            text = response.choices[0].message.content
            if not text:
                return "âš ï¸ OpenAI è¿”å›ç©ºæ–‡æœ¬"
            
            text = text.strip()
            logger.info("[Summarizer/OpenAI] æˆåŠŸç”Ÿæˆæ€»ç»“ (%d å­—ç¬¦)", len(text))
            return text

        except AttributeError as e:
            logger.error(
                "[Summarizer/OpenAI] å±æ€§è®¿é—®é”™è¯¯ (å¯èƒ½ API é…ç½®æœ‰è¯¯): %s, response type: %s",
                e, type(response).__name__ if 'response' in locals() else 'undefined'
            )
            return f"âš ï¸ OpenAI API é…ç½®é”™è¯¯: {str(e)}"
        except Exception as e:
            error_str = str(e)
            is_rate_limit = "429" in error_str or "rate_limit" in error_str.lower()

            if is_rate_limit and attempt < max_retries:
                delay = base_delay * (2 ** attempt)
                logger.warning(
                    "[Summarizer/OpenAI] Rate Limitï¼Œç¬¬ %d/%d æ¬¡é‡è¯•ï¼Œç­‰å¾… %ds...",
                    attempt + 1, max_retries, delay,
                )
                time.sleep(delay)
                continue

            logger.error("[Summarizer/OpenAI] è°ƒç”¨å¤±è´¥: %s", e, exc_info=True)
            return f"âš ï¸ OpenAI è°ƒç”¨å¤±è´¥: {type(e).__name__}"

    return "âš ï¸ OpenAI é‡è¯•æ¬¡æ•°è€—å°½"


def summarize(text_content: str) -> str:
    """
    è°ƒç”¨é…ç½®çš„ LLM æä¾›å•†å¯¹è®ºæ–‡æ–‡æœ¬è¿›è¡Œæ€»ç»“ã€‚

    Args:
        text_content: è®ºæ–‡æ‘˜è¦æˆ–å…¨æ–‡ç‰‡æ®µã€‚

    Returns:
        Markdown æ ¼å¼çš„ä¸­æ–‡ç®€æŠ¥ã€‚å¤±è´¥æ—¶è¿”å›å ä½æ–‡æœ¬ã€‚
    """
    if not text_content or not text_content.strip():
        return "_æ— å†…å®¹å¯ä¾›æ€»ç»“_"

    max_retries = config.LLM_MAX_RETRIES
    base_delay = config.LLM_RETRY_BASE_DELAY

    if config.LLM_PROVIDER == "gemini":
        return _summarize_with_gemini(text_content, max_retries, base_delay)
    elif config.LLM_PROVIDER == "deepseek":
        return _summarize_with_deepseek(text_content, max_retries, base_delay)
    elif config.LLM_PROVIDER == "openai":
        return _summarize_with_openai(text_content, max_retries, base_delay)
    else:
        logger.error("[Summarizer] æœªçŸ¥çš„ LLM_PROVIDER: %s", config.LLM_PROVIDER)
        return f"âš ï¸ é…ç½®é”™è¯¯: LLM_PROVIDER={config.LLM_PROVIDER}"
